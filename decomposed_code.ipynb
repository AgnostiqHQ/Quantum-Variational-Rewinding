{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbaker/opt/miniconda3/envs/algo-time-series/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import covalent as ct\n",
    "import pickle\n",
    "import os\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from itertools import combinations\n",
    "from pytorch_scipy_optimizer import Optimizer\n",
    "\n",
    "# Need this to resolve paths pater\n",
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def load_data_from_pickle(path):\n",
    "    data = pickle.load(open(path, 'rb'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = load_data_from_pickle(\"\".join([base_path, \"/data/Xtr.pickle\"])) # 3d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def data_santity_check(data):\n",
    "    # Only accept 3d data. (num_series, num_features, num_time_points)\n",
    "    X = np.asarray(data)\n",
    "    # Assume 3d array is list of nd-feature time series\n",
    "    if len(X.shape) != 3:\n",
    "        raise TypeError(\"Dimensions of X must be 3 dimensional: (num_series, num_features, num_time_points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_santity_check(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_series_training_dataloader(Xtr, n_series_batch, shuffle=True):\n",
    "    dataloader_series = \\\n",
    "        torch.utils.data.DataLoader(Xtr, batch_size=n_series_batch, shuffle=shuffle)\n",
    "    return dataloader_series\n",
    "\n",
    "@ct.electron\n",
    "def get_timepoint_training_dataloader(Xtr, n_t_batch, shuffle=True):\n",
    "    n_time_points = Xtr.shape[2]\n",
    "    data_loader_timeidxs = \\\n",
    "        torch.utils.data.DataLoader(torch.tensor(np.arange(n_time_points)), batch_size=n_t_batch, shuffle=True)\n",
    "    return data_loader_timeidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cycler = get_series_training_dataloader(Xtr, 10)\n",
    "t_cycler = get_timepoint_training_dataloader(Xtr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def arctan_penalty(sigma, contraction_hyperparameter):\n",
    "    prefac = 1/(np.pi)\n",
    "    sum_terms = torch.arctan(2*np.pi*contraction_hyperparameter*torch.abs(sigma))\n",
    "    mean = sum_terms.mean()\n",
    "    return prefac*mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([100, 100, 100])\n",
    "pen = arctan_penalty(sigma, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def create_diagonal_circuit(D, n, k=None):\n",
    "    if k is None:\n",
    "        k = n\n",
    "    cnt = 0\n",
    "    for i in range(1, k + 1):\n",
    "        for comb in combinations(range(n), i):\n",
    "            if len(comb) == 1:\n",
    "                qml.RZ(D[cnt], wires=[comb[0]])\n",
    "                cnt += 1\n",
    "            elif len(comb) > 1:\n",
    "                cnots = [comb[i : i + 2] for i in range(len(comb) - 1)]\n",
    "                for j in cnots:\n",
    "                    qml.CNOT(wires=j)\n",
    "                qml.RZ(D[cnt], wires=[comb[-1]])\n",
    "                cnt += 1\n",
    "                for j in cnots[::-1]:\n",
    "                    qml.CNOT(wires=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.tensor([2, 2, 2])\n",
    "n_qubits = 2\n",
    "create_diagonal_circuit(D, n_qubits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_device(n_qubits):\n",
    "    device = qml.device('lightning.qubit', wires=n_qubits, shots=None)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "@qml.qnode(device, interface='torch')\n",
    "def get_anomaly_expec(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                      embed_func_params={}, transform_func_params={}):\n",
    "    embed_func(x, wires=wires, **embed_func_params)\n",
    "    transform_func(alpha, wires, **transform_func_params)\n",
    "    diag_func(D * t, n_qubits, k=k)\n",
    "    qml.adjoint(transform_func)(alpha, wires=range(n_qubits), **transform_func_params\n",
    "        )\n",
    "    coeffs = np.ones(len(wires))/len(wires)\n",
    "    H = qml.Hamiltonian(coeffs, observable)\n",
    "    return qml.expval(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8834, dtype=torch.float64, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observable = [qml.PauliZ(i) for i in range(n_qubits)]\n",
    "x = torch.tensor([0.1, 0.7])\n",
    "t = torch.tensor([0.1])\n",
    "D = torch.tensor([0.1, 0.1, 0.1])\n",
    "k = 2\n",
    "transform_func = qml.templates.StronglyEntanglingLayers\n",
    "alpha_weights_shape = transform_func.shape(3, 2)\n",
    "alpha = torch.tensor(np.random.uniform(0, 2 * np.pi, size=alpha_weights_shape), requires_grad=True\n",
    "            ).type(torch.DoubleTensor)\n",
    "embed_func = qml.templates.AngleEmbedding\n",
    "\n",
    "get_anomaly_expec(x, t, D, alpha, range(n_qubits), k, embed_func, transform_func, create_diagonal_circuit, observable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def sample_D(sigma, mu):\n",
    "    D = torch.normal(mu, sigma.abs())\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = sample_D(torch.tensor([1.0, 2.0, 3.0]), torch.tensor([0.0, 3.1, 4.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_single_point_cost(x, t, alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                      embed_func_params={}, transform_func_params={}):\n",
    "    expecs = torch.zeros(N_E)\n",
    "    for i in range(N_E):\n",
    "        D = D_sample_func(sigma, mu)\n",
    "        expec = get_anomaly_expec(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                                   embed_func_params={}, transform_func_params={})\n",
    "        expecs[i] = expec\n",
    "    mean = expecs.mean()\n",
    "    single_point_cost = (q - mean)**2/4\n",
    "    return single_point_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8288], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.tensor([-1])\n",
    "mu = torch.tensor([0.1, 0.2, 0.1])\n",
    "sigma = torch.tensor([0.6, 4.1, 2.0])\n",
    "N_E = 10\n",
    "wires = range(n_qubits)\n",
    "diag_func = create_diagonal_circuit\n",
    "get_single_point_cost(x, t, alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func, transform_func, diag_func, observable,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_time_series_cost(xt, alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                         transform_func_params={}):\n",
    "    if t_cycler is None:\n",
    "        t_idxs = np.arange(xt.shape[1])\n",
    "        ts = np.linspace(0.1, 2 * np.pi, xt.shape[1], endpoint=True)\n",
    "    else:\n",
    "        t_idxs = next(t_cycler)\n",
    "        ts = np.linspace(0.1, 2 * np.pi, xt.shape[1], endpoint=True)[t_idxs]\n",
    "    xt_batch = xt[:, t_idxs]\n",
    "    xfunct = zip(xt_batch.T, ts)\n",
    "    a_func_t = \\\n",
    "        [get_single_point_cost(x, t, alpha, q, sample_D, sigma, mu, N_E, wires,\n",
    "                               k, embed_func, transform_func, diag_func, observable) for x, t in xfunct]\n",
    "    single_time_series_cost = torch.tensor(a_func_t, requires_grad=True).mean()\n",
    "    return single_time_series_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = Xtr[0]\n",
    "t_cycler = get_timepoint_training_dataloader(Xtr, 10)\n",
    "single_time_series_cost = \\\n",
    "     get_time_series_cost(xt, alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                         transform_func_params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_loss(alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "            transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau, embed_func_params={},\n",
    "            transform_func_params={}):\n",
    "\n",
    "    X_batch = next(x_cycler)\n",
    "    single_costs = torch.zeros(X_batch.shape[0])\n",
    "    for i, xt in enumerate(X_batch):\n",
    "        single_time_series_cost = \\\n",
    "            get_time_series_cost(xt, alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func,\n",
    "                                 transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                                 transform_func_params={})\n",
    "        single_costs[i] = single_time_series_cost\n",
    "    loss = 0.5*single_costs.mean() + penalty(sigma, tau)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = arctan_penalty\n",
    "tau = 1\n",
    "loss = get_loss(alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func,\n",
    "            transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_initial_parameters(transform_func, n_qubits, transform_func_layers):\n",
    "    initial_alpha =\\\n",
    "         torch.tensor(np.random.uniform(0, 2 * np.pi, size=transform_func.shape(transform_func_layers, n_qubits)),\n",
    "         requires_grad=True).type(torch.DoubleTensor)\n",
    "    initial_mu = torch.tensor(\n",
    "                np.random.uniform(0, 2 * np.pi, 2 ** (n_qubits) - 1)).type(torch.DoubleTensor)\n",
    "    initial_sigma = torch.tensor(\n",
    "                np.random.uniform(0, 2 * np.pi, 2 ** (n_qubits) - 1)).type(torch.DoubleTensor)\n",
    "\n",
    "    initial_q = torch.tensor(np.random.uniform(-1, 1)).type(torch.DoubleTensor)\n",
    "    init_parameters = {'alpha': initial_alpha, 'mu': initial_mu, 'sigma': initial_sigma, 'q': initial_q}\n",
    "    return init_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': tensor([[[2.6582, 4.0949, 2.5982],\n",
      "         [3.1192, 4.8353, 3.2766]],\n",
      "\n",
      "        [[0.2184, 3.6168, 4.4182],\n",
      "         [0.9721, 0.4044, 5.0335]],\n",
      "\n",
      "        [[2.2754, 3.4881, 0.4931],\n",
      "         [2.4994, 3.5556, 0.5866]]], dtype=torch.float64, requires_grad=True), 'mu': tensor([5.9833, 3.9248, 0.2735], dtype=torch.float64), 'sigma': tensor([3.7458, 0.5436, 1.4291], dtype=torch.float64), 'q': tensor(-0.7046, dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "transform_func = qml.templates.StronglyEntanglingLayers\n",
    "n_qubits = 2\n",
    "transform_func_layers = 3\n",
    "init_parameters = get_initial_parameters(transform_func, n_qubits, transform_func_layers)\n",
    "print(init_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_model(alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func, n_qubits,\n",
    "                transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau,\n",
    "                optimizer_params, initial_parameters):\n",
    "    f = lambda alpha, mu, sigma, q: get_loss(alpha=alpha, mu=mu, sigma=sigma, q=q,\n",
    "                                             D_sample_func=D_sample_func, N_E=N_E, wires=range(n_qubits),\n",
    "                                             k=k, embed_func=embed_func, transform_func=transform_func,\n",
    "                                             diag_func=diag_func, observable=observable,\n",
    "                                             t_cycler=t_cycler, x_cycler=x_cycler,\n",
    "                                             penalty=penalty, tau=tau)\n",
    "    optimizer = Optimizer(f, initial_parameters, optimizer_params)\n",
    "    opt_params = optimizer.optimize()\n",
    "    return {\"opt_params\": opt_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Return from subroutine COBYLA because the MAXFUN limit has been reached.\n",
      "\n",
      "   NFVALS =    1   F = 5.150300E-01    MAXCV = 0.000000E+00\n",
      "   X = 6.689771E-01   1.275315E+00   9.910960E-01   5.463059E+00   3.674307E+00\n",
      "       3.371600E+00   6.075654E+00   9.679636E-01   4.447453E-01   3.816727E+00\n",
      "       4.746373E-01   4.670682E+00   4.289763E+00   2.150155E+00   2.918334E+00\n",
      "       5.595792E+00   4.015374E+00   2.052411E+00   5.432162E+00   3.549622E+00\n",
      "       1.667371E+00   5.858200E+00   1.560359E+00   2.256683E+00   6.281301E-01\n",
      "{'opt_params': {'alpha': tensor([[[0.6690, 1.2753, 0.9911],\n",
      "         [5.4631, 3.6743, 3.3716]],\n",
      "\n",
      "        [[6.0757, 0.9680, 0.4447],\n",
      "         [3.8167, 0.4746, 4.6707]],\n",
      "\n",
      "        [[4.2898, 2.1502, 2.9183],\n",
      "         [5.5958, 4.0154, 2.0524]]], dtype=torch.float64, requires_grad=True), 'mu': tensor([5.4322, 3.5496, 1.6674], dtype=torch.float64), 'sigma': tensor([5.8582, 1.5604, 2.2567], dtype=torch.float64), 'q': tensor(0.6281, dtype=torch.float64)}}\n"
     ]
    }
   ],
   "source": [
    "initial_parameters = get_initial_parameters(transform_func, n_qubits, transform_func_layers)\n",
    "optimizer_params = {\n",
    "                \"method\": \"COBYLA\",\n",
    "                \"options\": {\"disp\": True, \"maxiter\": 1},\n",
    "                \"jac\": False,\n",
    "            }\n",
    "opt_params =\\\n",
    "    train_model(alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func, n_qubits,\n",
    "                transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau,\n",
    "                optimizer_params, initial_parameters)\n",
    "print(opt_params)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': tensor([[[0.6690, 1.2753, 0.9911],\n",
       "          [5.4631, 3.6743, 3.3716]],\n",
       " \n",
       "         [[6.0757, 0.9680, 0.4447],\n",
       "          [3.8167, 0.4746, 4.6707]],\n",
       " \n",
       "         [[4.2898, 2.1502, 2.9183],\n",
       "          [5.5958, 4.0154, 2.0524]]], dtype=torch.float64, requires_grad=True),\n",
       " 'mu': tensor([5.4322, 3.5496, 1.6674], dtype=torch.float64),\n",
       " 'sigma': tensor([5.8582, 1.5604, 2.2567], dtype=torch.float64),\n",
       " 'q': tensor(0.6281, dtype=torch.float64)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params['opt_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_anomaly_scores(Xte, normal_cost, penalty, tau, results_dict, D_sample_func, wires, k, embed_func,\n",
    "                       transform_func, diag_func, observable, get_time_resolved=False):\n",
    "    opt_params = results_dict['opt_params']\n",
    "    alpha = opt_params['alpha']\n",
    "    mu = opt_params['mu']\n",
    "    sigma = opt_params['sigma']\n",
    "    q = opt_params['q']\n",
    "    scores = torch.zeros(Xte.shape[0])\n",
    "    for i, yt in enumerate(Xte):\n",
    "        single_time_series_cost =\\\n",
    "            get_time_series_cost(yt, alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler=None)\n",
    "        scores[i] = (normal_cost - single_time_series_cost - penalty(sigma, tau)).abs()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 1\n",
      "done 2\n",
      "done 3\n",
      "done 4\n",
      "done 5\n",
      "done 6\n",
      "done 7\n",
      "done 8\n",
      "done 9\n",
      "done 10\n",
      "done 11\n",
      "done 12\n",
      "done 13\n",
      "done 14\n",
      "done 15\n",
      "done 16\n",
      "done 17\n",
      "done 18\n",
      "done 19\n",
      "done 20\n",
      "done 21\n",
      "done 22\n",
      "done 23\n",
      "done 24\n",
      "done 25\n",
      "done 26\n",
      "done 27\n",
      "done 28\n",
      "done 29\n",
      "done 30\n",
      "done 31\n",
      "done 32\n",
      "done 33\n",
      "done 34\n",
      "done 35\n",
      "done 36\n",
      "done 37\n",
      "done 38\n",
      "done 39\n",
      "done 40\n",
      "done 41\n",
      "done 42\n",
      "done 43\n",
      "done 44\n",
      "done 45\n",
      "done 46\n",
      "done 47\n",
      "done 48\n",
      "done 49\n",
      "done 50\n",
      "done 51\n",
      "done 52\n",
      "done 53\n",
      "done 54\n",
      "done 55\n",
      "done 56\n",
      "done 57\n",
      "done 58\n",
      "done 59\n"
     ]
    }
   ],
   "source": [
    "Yte = load_data_from_pickle(\"\".join([base_path, \"/data/Xte_dirty_usdt_pos.pickle\"]))\n",
    "Yte = Yte[:, :, ::4]\n",
    "normal_cost = 0\n",
    "D_sample_func = sample_D\n",
    "k = 2\n",
    "n_qubits = 2\n",
    "transform_func = qml.templates.StronglyEntanglingLayers\n",
    "diag_func = create_diagonal_circuit\n",
    "wires = range(n_qubits)\n",
    "scores = get_anomaly_scores(Yte, normal_cost, penalty, tau, opt_params, D_sample_func, wires, k, embed_func,\n",
    "                            transform_func, diag_func, observable, get_time_resolved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5463, 0.5453, 0.5519, 0.5513, 0.5511, 0.5550, 0.5504, 0.5424, 0.5447,\n",
      "        0.5494, 0.5538, 0.5511, 0.5338, 0.5447, 0.5448, 0.5509, 0.5481, 0.5448,\n",
      "        0.5456, 0.5381, 0.5471, 0.5554, 0.5487, 0.5499, 0.5474, 0.5495, 0.5467,\n",
      "        0.5494, 0.5497, 0.5477, 0.5500, 0.5525, 0.5551, 0.5479, 0.5517, 0.5512,\n",
      "        0.5426, 0.5491, 0.5670, 0.5452, 0.5486, 0.5537, 0.5500, 0.5474, 0.5535,\n",
      "        0.5459, 0.5499, 0.5487, 0.5445, 0.5491, 0.5644, 0.5460, 0.5480, 0.5540,\n",
      "        0.5515, 0.5490, 0.5531, 0.5443, 0.5452, 0.5528], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_transform_func(type):\n",
    "    if type == 'sel':\n",
    "        transform_func = qml.StronglyEntanglingLayers\n",
    "    return transform_func\n",
    "\n",
    "@ct.electron\n",
    "def get_embedding_func(type):\n",
    "    if type == 'ry':\n",
    "        embed_func = qml.templates.AngleEmbedding\n",
    "    return embed_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrons all work locally, let's dispatch them at lattices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def training_workflow(Xtr_path, n_series_batch, n_t_batch, transform_func_type, n_qubits, transform_func_layers, transform_func):\n",
    "    # load training data\n",
    "    Xtr = load_data_from_pickle(Xtr_path)\n",
    "\n",
    "    # check dimensions of input data are correct\n",
    "    data_santity_check(Xtr)\n",
    "\n",
    "    # Get dataloaders for series and time-point batches\n",
    "    series_dataloader = get_series_training_dataloader(Xtr, n_series_batch)\n",
    "    time_dataloader = get_timepoint_training_dataloader(Xtr, n_t_batch)\n",
    "\n",
    "    # get penalty function\n",
    "    pen = arctan_penalty\n",
    "\n",
    "    # get transform_func\n",
    "    transform_func = get_transform_func(transform_func_type)\n",
    "\n",
    "    # get embedding func\n",
    "    embed_func = get_embedding_func\n",
    "\n",
    "    # get random initial parameters\n",
    "    init_parameters = get_initial_parameters(transform_func, n_qubits, transform_func_layers)\n",
    "\n",
    "    # get \n",
    "    \n",
    "    return pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type property is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jbaker/Code/workshops/QuantumVariationalRewinding/decomposed_code.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbaker/Code/workshops/QuantumVariationalRewinding/decomposed_code.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Xtr_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([base_path, \u001b[39m\"\u001b[39m\u001b[39m/data/Xtr.pickle\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jbaker/Code/workshops/QuantumVariationalRewinding/decomposed_code.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dispatch_id \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39;49mdispatch(training_workflow)(Xtr_path, \u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, qml\u001b[39m.\u001b[39;49mtemplates\u001b[39m.\u001b[39;49mStronglyEntanglingLayers)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbaker/Code/workshops/QuantumVariationalRewinding/decomposed_code.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ct_results \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mget_result(dispatch_id\u001b[39m=\u001b[39mdispatch_id, wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jbaker/Code/workshops/QuantumVariationalRewinding/decomposed_code.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m Xtr \u001b[39m=\u001b[39m ct_results\u001b[39m.\u001b[39mresult\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/algo-time-series/lib/python3.10/site-packages/covalent/_dispatcher_plugins/local.py:84\u001b[0m, in \u001b[0;36mLocalDispatcher.dispatch.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m lattice\u001b[39m.\u001b[39mbuild_graph(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[39m# Serialize the transport graph to JSON\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m json_lattice \u001b[39m=\u001b[39m lattice\u001b[39m.\u001b[39;49mserialize_to_json()\n\u001b[1;32m     86\u001b[0m test_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m{\u001b[39;00mdispatcher_addr\u001b[39m}\u001b[39;00m\u001b[39m/api/submit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(test_url, data\u001b[39m=\u001b[39mjson_lattice)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/algo-time-series/lib/python3.10/site-packages/covalent/_workflow/lattice.py:134\u001b[0m, in \u001b[0;36mLattice.serialize_to_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m attributes[\u001b[39m\"\u001b[39m\u001b[39mcova_imports\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcova_imports)\n\u001b[1;32m    131\u001b[0m \u001b[39m# for k, v in attributes.items():\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m#     print(k, type(v))\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mdumps(attributes)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/algo-time-series/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/algo-time-series/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/algo-time-series/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/algo-time-series/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type property is not JSON serializable"
     ]
    }
   ],
   "source": [
    "Xtr_path = \"\".join([base_path, \"/data/Xtr.pickle\"])\n",
    "\n",
    "dispatch_id = ct.dispatch(training_workflow)(Xtr_path, 10, 10, 'sel', 2, 3, qml.templates.StronglyEntanglingLayers)\n",
    "\n",
    "ct_results = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "Xtr = ct_results.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct', 'electron'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_workflow.cova_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_workflow(Xtr_path, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('algo-time-series')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebe333cbe530cfdcf63924802829da59b1d6692df1c9241ebe53f375aea617cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
