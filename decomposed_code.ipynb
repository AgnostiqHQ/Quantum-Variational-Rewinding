{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "import pickle\n",
    "import os\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from itertools import combinations\n",
    "from pytorch_scipy_optimizer import Optimizer\n",
    "\n",
    "# Need this to resolve paths pater\n",
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def load_data_from_pickle(path):\n",
    "    data = pickle.load(open(path, 'rb'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = load_data_from_pickle(\"\".join([base_path, \"/data/Xtr.pickle\"])) # 3d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def data_santity_check(data):\n",
    "    # Only accept 3d data. (num_series, num_features, num_time_points)\n",
    "    X = np.asarray(data)\n",
    "    # Assume 3d array is list of nd-feature time series\n",
    "    if len(X.shape) != 3:\n",
    "        raise TypeError(\"Dimensions of X must be 3 dimensional: (num_series, num_features, num_time_points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_santity_check(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "    def __init__(self, X, batch_size, n_batch_iterations=1000):\n",
    "        self.data = []\n",
    "        self._init_data(\n",
    "            cycle(torch.utils.data.DataLoader(X, batch_size=batch_size, shuffle=True)),\n",
    "            n_batch_iterations)\n",
    "\n",
    "    def _init_data(self, iterator, n_batch_iterations=1000):\n",
    "        x = next(iterator, None)\n",
    "        for i in range(n_batch_iterations):\n",
    "            self.data.append(x)\n",
    "            x = next(iterator, None)\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.data.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_series_training_dataloader(Xtr, n_series_batch, batch_iterations=1000):\n",
    "    x_cycler = DataGetter(Xtr, n_series_batch, batch_iterations)\n",
    "    return x_cycler\n",
    "\n",
    "@ct.electron\n",
    "def get_timepoint_training_dataloader(Xtr, n_t_batch, batch_iterations=1000):\n",
    "    n_time_points = Xtr.shape[2]\n",
    "    T  = torch.tensor(np.arange(n_time_points))\n",
    "    t_cycler = DataGetter(T, n_t_batch, batch_iterations)\n",
    "    return t_cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cycler = get_series_training_dataloader(Xtr, 10)\n",
    "t_cycler = get_timepoint_training_dataloader(Xtr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def arctan_penalty(sigma, contraction_hyperparameter):\n",
    "    prefac = 1/(np.pi)\n",
    "    sum_terms = torch.arctan(2*np.pi*contraction_hyperparameter*torch.abs(sigma))\n",
    "    mean = sum_terms.mean()\n",
    "    return prefac*mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([100, 100, 100])\n",
    "pen = arctan_penalty(sigma, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def create_diagonal_circuit(D, n, k=None):\n",
    "    if k is None:\n",
    "        k = n\n",
    "    cnt = 0\n",
    "    for i in range(1, k + 1):\n",
    "        for comb in combinations(range(n), i):\n",
    "            if len(comb) == 1:\n",
    "                qml.RZ(D[cnt], wires=[comb[0]])\n",
    "                cnt += 1\n",
    "            elif len(comb) > 1:\n",
    "                cnots = [comb[i : i + 2] for i in range(len(comb) - 1)]\n",
    "                for j in cnots:\n",
    "                    qml.CNOT(wires=j)\n",
    "                qml.RZ(D[cnt], wires=[comb[-1]])\n",
    "                cnt += 1\n",
    "                for j in cnots[::-1]:\n",
    "                    qml.CNOT(wires=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.tensor([2, 2, 2])\n",
    "n_qubits = 2\n",
    "create_diagonal_circuit(D, n_qubits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_device(n_qubits):\n",
    "    device = qml.device('default.qubit', wires=n_qubits, shots=None)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m@ct\u001b[39m\u001b[39m.\u001b[39melectron\n\u001b[0;32m----> 2\u001b[0m \u001b[39m@qml\u001b[39m\u001b[39m.\u001b[39mqnode(device, interface\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_anomaly_expec\u001b[39m(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n\u001b[1;32m      4\u001b[0m                       embed_func_params\u001b[39m=\u001b[39m{}, transform_func_params\u001b[39m=\u001b[39m{}):\n\u001b[1;32m      5\u001b[0m     embed_func(x, wires\u001b[39m=\u001b[39mwires, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39membed_func_params)\n\u001b[1;32m      6\u001b[0m     transform_func(alpha, wires, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtransform_func_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "@ct.electron\n",
    "@qml.qnode(device, interface='torch')\n",
    "def get_anomaly_expec(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                      embed_func_params={}, transform_func_params={}):\n",
    "    embed_func(x, wires=wires, **embed_func_params)\n",
    "    transform_func(alpha, wires, **transform_func_params)\n",
    "    diag_func(D * t, n_qubits, k=k)\n",
    "    qml.adjoint(transform_func)(alpha, wires=range(n_qubits), **transform_func_params\n",
    "        )\n",
    "    coeffs = np.ones(len(wires))/len(wires)\n",
    "    H = qml.Hamiltonian(coeffs, observable)\n",
    "    return qml.expval(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observable = [qml.PauliZ(i) for i in range(n_qubits)]\n",
    "x = torch.tensor([0.1, 0.7])\n",
    "t = torch.tensor([0.1])\n",
    "D = torch.tensor([0.1, 0.1, 0.1])\n",
    "k = 2\n",
    "transform_func = qml.templates.StronglyEntanglingLayers\n",
    "alpha_weights_shape = transform_func.shape(3, 2)\n",
    "alpha = torch.tensor(np.random.uniform(0, 2 * np.pi, size=alpha_weights_shape), requires_grad=True\n",
    "            ).type(torch.DoubleTensor)\n",
    "embed_func = qml.templates.AngleEmbedding\n",
    "\n",
    "get_anomaly_expec(x, t, D, alpha, range(n_qubits), k, embed_func, transform_func, create_diagonal_circuit, observable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def sample_D(sigma, mu):\n",
    "    D = torch.normal(mu, sigma.abs())\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = sample_D(torch.tensor([1.0, 2.0, 3.0]), torch.tensor([0.0, 3.1, 4.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_single_point_cost(x, t, alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                      embed_func_params={}, transform_func_params={}):\n",
    "    expecs = torch.zeros(N_E)\n",
    "    for i in range(N_E):\n",
    "        D = D_sample_func(sigma, mu)\n",
    "        expec = get_anomaly_expec(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                                   embed_func_params={}, transform_func_params={})\n",
    "        expecs[i] = expec\n",
    "    mean = expecs.mean()\n",
    "    single_point_cost = (q - mean)**2/4\n",
    "    return single_point_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.tensor([-1])\n",
    "mu = torch.tensor([0.1, 0.2, 0.1])\n",
    "sigma = torch.tensor([0.6, 4.1, 2.0])\n",
    "N_E = 10\n",
    "wires = range(n_qubits)\n",
    "diag_func = create_diagonal_circuit\n",
    "get_single_point_cost(x, t, alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func, transform_func, diag_func, observable,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_time_series_cost(xt, alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                         transform_func_params={}):\n",
    "    if t_cycler is None:\n",
    "        t_idxs = np.arange(xt.shape[1])\n",
    "        ts = np.linspace(0.1, 2 * np.pi, xt.shape[1], endpoint=True)\n",
    "    else:\n",
    "        t_idxs = next(t_cycler)\n",
    "        ts = np.linspace(0.1, 2 * np.pi, xt.shape[1], endpoint=True)[t_idxs]\n",
    "    xt_batch = xt[:, t_idxs]\n",
    "    xfunct = zip(xt_batch.T, ts)\n",
    "    a_func_t = \\\n",
    "        [get_single_point_cost(x, t, alpha, q, sample_D, sigma, mu, N_E, wires,\n",
    "                               k, embed_func, transform_func, diag_func, observable) for x, t in xfunct]\n",
    "    single_time_series_cost = torch.tensor(a_func_t, requires_grad=True).mean()\n",
    "    return single_time_series_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = Xtr[0]\n",
    "t_cycler = get_timepoint_training_dataloader(Xtr, 10)\n",
    "single_time_series_cost = \\\n",
    "     get_time_series_cost(xt, alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                         transform_func_params={})\n",
    "print(single_time_series_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_loss(alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "            transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau, embed_func_params={},\n",
    "            transform_func_params={}):\n",
    "\n",
    "    X_batch = next(x_cycler)\n",
    "    single_costs = torch.zeros(X_batch.shape[0])\n",
    "    for i, xt in enumerate(X_batch):\n",
    "        single_time_series_cost = \\\n",
    "            get_time_series_cost(xt, alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func,\n",
    "                                 transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                                 transform_func_params={})\n",
    "        single_costs[i] = single_time_series_cost\n",
    "    loss = 0.5*single_costs.mean() + penalty(sigma, tau)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = arctan_penalty\n",
    "tau = 1\n",
    "loss = get_loss(alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func,\n",
    "            transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_initial_parameters(transform_func_shape, n_qubits):\n",
    "    initial_alpha =\\\n",
    "         torch.tensor(np.random.uniform(0, 2 * np.pi, size=transform_func_shape),\n",
    "         requires_grad=True).type(torch.DoubleTensor)\n",
    "    initial_mu = torch.tensor(\n",
    "                np.random.uniform(0, 2 * np.pi, 2 ** (n_qubits) - 1)).type(torch.DoubleTensor)\n",
    "    initial_sigma = torch.tensor(\n",
    "                np.random.uniform(0, 2 * np.pi, 2 ** (n_qubits) - 1)).type(torch.DoubleTensor)\n",
    "\n",
    "    initial_q = torch.tensor(np.random.uniform(-1, 1)).type(torch.DoubleTensor)\n",
    "    init_parameters = {'alpha': initial_alpha, 'mu': initial_mu, 'sigma': initial_sigma, 'q': initial_q}\n",
    "    return init_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 2\n",
    "transform_func_layers = 3\n",
    "transform_func_shape = qml.templates.StronglyEntanglingLayers.shape(transform_func_layers, n_qubits)\n",
    "init_parameters = get_initial_parameters(transform_func_shape, n_qubits)\n",
    "print(init_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_model(alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func, n_qubits,\n",
    "                transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau,\n",
    "                optimizer_params, initial_parameters):\n",
    "    f = lambda alpha, mu, sigma, q: get_loss(alpha=alpha, mu=mu, sigma=sigma, q=q,\n",
    "                                             D_sample_func=D_sample_func, N_E=N_E, wires=range(n_qubits),\n",
    "                                             k=k, embed_func=embed_func, transform_func=transform_func,\n",
    "                                             diag_func=diag_func, observable=observable,\n",
    "                                             t_cycler=t_cycler, x_cycler=x_cycler,\n",
    "                                             penalty=penalty, tau=tau)\n",
    "    optimizer = Optimizer(f, initial_parameters, optimizer_params)\n",
    "    opt_params = optimizer.optimize()\n",
    "    return {\"opt_params\": opt_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_parameters = get_initial_parameters(transform_func_shape, n_qubits)\n",
    "optimizer_params = {\n",
    "                \"method\": \"COBYLA\",\n",
    "                \"options\": {\"disp\": True, \"maxiter\": 1},\n",
    "                \"jac\": False,\n",
    "            }\n",
    "opt_params =\\\n",
    "    train_model(alpha, q, sample_D, sigma, mu, N_E, wires, k, embed_func, n_qubits,\n",
    "                transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau,\n",
    "                optimizer_params, initial_parameters)\n",
    "print(opt_params)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params['opt_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_anomaly_scores(Xte, normal_cost, penalty, tau, results_dict, D_sample_func, wires, k, embed_func,\n",
    "                       transform_func, diag_func, observable, get_time_resolved=False):\n",
    "    opt_params = results_dict['opt_params']\n",
    "    alpha = opt_params['alpha']\n",
    "    mu = opt_params['mu']\n",
    "    sigma = opt_params['sigma']\n",
    "    q = opt_params['q']\n",
    "    scores = torch.zeros(Xte.shape[0])\n",
    "    for i, yt in enumerate(Xte):\n",
    "        single_time_series_cost =\\\n",
    "            get_time_series_cost(yt, alpha, q, D_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler=None)\n",
    "        scores[i] = (normal_cost - single_time_series_cost - penalty(sigma, tau)).abs()\n",
    "        print('done', i)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yte = load_data_from_pickle(\"\".join([base_path, \"/data/Xte_dirty_usdt_pos.pickle\"]))\n",
    "Yte = Yte[1:3, :, ::4]\n",
    "normal_cost = 0\n",
    "D_sample_func = sample_D\n",
    "k = 2\n",
    "n_qubits = 2\n",
    "transform_func = qml.templates.StronglyEntanglingLayers\n",
    "diag_func = create_diagonal_circuit\n",
    "wires = range(n_qubits)\n",
    "scores = get_anomaly_scores(Yte, normal_cost, penalty, tau, opt_params, D_sample_func, wires, k, embed_func,\n",
    "                            transform_func, diag_func, observable, get_time_resolved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_transform_func(type):\n",
    "    if type == 'sel':\n",
    "        transform_func = qml.StronglyEntanglingLayers\n",
    "    return transform_func\n",
    "\n",
    "@ct.electron\n",
    "def get_embedding_func(type):\n",
    "    if type == 'rx':\n",
    "        embed_func = qml.templates.AngleEmbedding\n",
    "    return embed_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrons all work locally, let's dispatch them at lattices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def training_workflow(Xtr_path, n_series_batch, n_t_batch, transform_func_type, n_qubits, transform_func_layers,\n",
    "                      embed_func_type='rx'):\n",
    "    # load training data\n",
    "    Xtr = load_data_from_pickle(Xtr_path)\n",
    "\n",
    "    # check dimensions of input data are correct\n",
    "    data_santity_check(Xtr)\n",
    "\n",
    "    # Get dataloaders for series and time-point batches\n",
    "    x_cycler = get_series_training_dataloader(Xtr, n_series_batch)\n",
    "    t_cycler = get_timepoint_training_dataloader(Xtr, n_t_batch)\n",
    "\n",
    "    # get penalty function\n",
    "    pen = arctan_penalty\n",
    "\n",
    "    # get transform_func\n",
    "    transform_func = get_transform_func(transform_func_type)\n",
    "\n",
    "    # get embedding func\n",
    "    embed_func = get_embedding_func(embed_func_type)\n",
    "\n",
    "    # get random initial parameters\n",
    "    init_parameters = get_initial_parameters(transform_func.shape(transform_func_layers, n_qubits), n_qubits)    \n",
    "    return pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: http://localhost:48008/api/submit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m Xtr_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([base_path, \u001b[39m\"\u001b[39m\u001b[39m/data/Xtr.pickle\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m dispatch_id \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39;49mdispatch(training_workflow)(Xtr_path, \u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m ct_results \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mget_result(dispatch_id\u001b[39m=\u001b[39mdispatch_id, wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m Xtr \u001b[39m=\u001b[39m ct_results\u001b[39m.\u001b[39mresult\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/QVR/lib/python3.8/site-packages/covalent/_dispatcher_plugins/local.py:87\u001b[0m, in \u001b[0;36mLocalDispatcher.dispatch.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m test_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m{\u001b[39;00mdispatcher_addr\u001b[39m}\u001b[39;00m\u001b[39m/api/submit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(test_url, data\u001b[39m=\u001b[39mjson_lattice)\n\u001b[0;32m---> 87\u001b[0m r\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/QVR/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:48008/api/submit"
     ]
    }
   ],
   "source": [
    "Xtr_path = \"\".join([base_path, \"/data/Xtr.pickle\"])\n",
    "\n",
    "dispatch_id = ct.dispatch(training_workflow)(Xtr_path, 10, 10, 'sel', 2, 3, 'rx')\n",
    "\n",
    "ct_results = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "Xtr = ct_results.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_workflow.cova_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_path = \"\".join([base_path, \"/data/Xtr.pickle\"])\n",
    "x = training_workflow(Xtr_path, 10, 10, 'sel', 2, 3, 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('QVR')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61006857bd2735ccfd2a6b3291596ddfb06066dbc255c4e4c64dd84bb5990a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
