{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Variational Rewinding for Time Series Anomaly Detection\n",
    "\n",
    "In this notebook, we show step-by-step the components of the Quantum Variational Rewinding (QVR) algorithm with explicit code. Then, using the open-source heterogenous workflow management tool - `Covalent` - we stitch together these components to create a working code.\n",
    "\n",
    "Using this workflow, we employ QVR to train and classify bivariate crpytocurrency time-series which were presented and expained in the main article.\n",
    "\n",
    "First, let's do all fo the neccessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "import pickle\n",
    "import os\n",
    "import pennylane.numpy as np\n",
    "import numpy as np_norm\n",
    "import torch\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "# from utils import Optimizer\n",
    "from pytorch_minimize.optim import MinimizeWrapper\n",
    "\n",
    "# Need this to resolve paths pater\n",
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "All of the relevant data is in the `./data/` directory in this repository. Let's create a `@ct.electron` to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def load_data_from_pickle(path):\n",
    "    data = pickle.load(open(path, 'rb'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for good measure (in case you wish to experiment with code), a sanity check on the format of the training data we are using. We require 3D arrays with dimensions $(m, d, p)$, where (consistent with the article) are:\n",
    "\n",
    "$m$: The total number of time series in the training dataset $X$\n",
    "\n",
    "$d$: The dimension of the time series\n",
    "\n",
    "$p$: The number of discrete time points in the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def data_santity_check(data):\n",
    "    X = np.asarray(data)\n",
    "    # Only accept 3d data. (m, d, p)\n",
    "    if len(X.shape) != 3:\n",
    "        raise TypeError(\"Dimensions of X must be 3 dimensional: (num_series, num_features, num_time_points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training with mini-batches, the data set is processed with a `torch` `Dataloader` object. We will use a helper class and and two `@ct.electron`s to do this (two because we have a batch approximation on the number of time points and on the number of time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter:\n",
    "    def __init__(self, X, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = []\n",
    "        self.X = X\n",
    "        self._init_data(\n",
    "            iter(\n",
    "                torch.utils.data.DataLoader(\n",
    "                    self.X, batch_size=self.batch_size, shuffle=True)))\n",
    "\n",
    "    def _init_data(self, iterator):\n",
    "        x = next(iterator, None)\n",
    "        while x is not None:\n",
    "            self.data.append(x)\n",
    "            x = next(iterator, None)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            return self.data.pop()\n",
    "        except(IndexError):\n",
    "            self._init_data(\n",
    "                iter(\n",
    "                    torch.utils.data.DataLoader(\n",
    "                        self.X, batch_size=self.batch_size, shuffle=True)))\n",
    "            return self.data.pop()\n",
    "\n",
    "@ct.electron\n",
    "def get_series_training_cycler(Xtr, n_series_batch):\n",
    "    x_cycler = DataGetter(Xtr, n_series_batch)\n",
    "    return x_cycler\n",
    "\n",
    "@ct.electron\n",
    "def get_timepoint_training_cycler(Xtr, n_t_batch):\n",
    "    n_time_points = Xtr.shape[2]\n",
    "    T  = torch.tensor(np.arange(n_time_points))\n",
    "    t_cycler = DataGetter(T, n_t_batch)\n",
    "    return t_cycler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the cost function\n",
    "\n",
    "Now we need `@ct.electron`'s to define the quantum circuits prepare the $n$-qubit state\n",
    "\n",
    "$$\n",
    "|\\bm{x}_i(t_j), \\bm{\\alpha}, \\bm{\\epsilon} \\rangle = W^{\\dagger}(\\bm{\\alpha})D(\\bm{\\epsilon}, t_j)W(\\bm{\\alpha})U[\\bm{x}_i(t_j)]|0 \\rangle ^{\\otimes n}\n",
    "$$\n",
    "\n",
    "and estimate the expecation value\n",
    "\n",
    "$$\n",
    "\\Omega \\left(\\bm{x}_i(t_j), \\bm{\\alpha}, \\bm{\\epsilon}, \\bm{\\eta} \\right) :=  \\langle \\bm{x}_i(t_j), \\bm{\\alpha}, \\bm{\\epsilon}|\\hat{O}_{\\bm{\\eta}}|\\bm{x}_i(t_j), \\bm{\\alpha}, \\bm{\\epsilon} \\rangle\n",
    "$$\n",
    "\n",
    "Where we chose in the article that \n",
    "\n",
    "$$\n",
    "\\hat{O}_{\\bm{\\eta}} = \\eta_0 I - \\frac 1n \\sum_{i=1}^{n} \\eta_i \\hat{\\sigma}_z^i\n",
    "$$\n",
    "\n",
    "where $\\eta_i = 1$ when $i > 0$\n",
    "\n",
    "The template for the above is the below `@ct.electron`, which leverages `Pennylane` and its interface with `Pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work explicitly with 2 qubits. This corresponds to the 'bivariate model' in the article\n",
    "n_qubits = 2\n",
    "\n",
    "@ct.electron\n",
    "@qml.qnode(qml.device('lightning.qubit', wires=n_qubits, shots=None), interface='torch')\n",
    "def get_anomaly_expec(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                      embed_func_params={}, transform_func_params={}):\n",
    "\n",
    "    embed_func(x, wires=wires, **embed_func_params) # U[x_i(t_j)]\n",
    "    transform_func(alpha, wires, **transform_func_params) # W(\\alpha)\n",
    "    diag_func(D * t, n_qubits, k=k) # D(\\epsilon, t_j)\n",
    "    qml.adjoint(transform_func)(alpha, wires=range(n_qubits), **transform_func_params\n",
    "        ) # W^{\\dagger}(\\alpha)\n",
    "    \n",
    "    # plug in \\hat{O}_{\\eta}\n",
    "    coeffs = np.ones(len(wires))/len(wires) # scale by 1/n\n",
    "    H = qml.Hamiltonian(coeffs, observable)\n",
    "     # calculate expecation\n",
    "    return qml.expval(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While $W(\\bm{\\alpha})$ and $U[\\bm{x}_i(t_j)]$ can be implemented with quantum circuits in the `Pennylane` templates library, we implement our own code for $D(\\bm{\\epsilon}, t_j)$ via a $k$-local approximation of \n",
    "$e^{-iM(\\epsilon)}$ (see https://dx.doi.org/10.1088/1367-2630/16/3/033040 for explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def create_diagonal_circuit(D, n, k=None):\n",
    "    # D is a sub-set of the exponential number of eigenvalues.\n",
    "    if k is None:\n",
    "        k = n\n",
    "    cnt = 0\n",
    "    # Note there NO explicit loop over all 2^n eigenvalues\n",
    "    for i in range(1, k + 1):\n",
    "        for comb in combinations(range(n), i):\n",
    "            if len(comb) == 1:\n",
    "                qml.RZ(D[cnt], wires=[comb[0]])\n",
    "                cnt += 1\n",
    "            elif len(comb) > 1:\n",
    "                cnots = [comb[i : i + 2] for i in range(len(comb) - 1)]\n",
    "                for j in cnots:\n",
    "                    qml.CNOT(wires=j)\n",
    "                qml.RZ(D[cnt], wires=[comb[-1]])\n",
    "                cnt += 1\n",
    "                for j in cnots[::-1]:\n",
    "                    qml.CNOT(wires=j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `get_anomaly_expec` to build $C_1(\\bm{x}_i(t_j), \\bm{\\theta})$, where $\\bm{\\theta} = [\\bm{\\alpha}, \\bm{\\mu}, \\bm{\\sigma}, \\bm{\\eta}]$ from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_single_point_cost(x, t, alpha, eta_0, M_sample_func, sigma, mu, N_E, wires,\n",
    "                          k, embed_func, transform_func, diag_func, observable,\n",
    "                          embed_func_params={}, transform_func_params={}):\n",
    "    expecs = torch.zeros(N_E)\n",
    "    for i in range(N_E):\n",
    "        D = M_sample_func(sigma, mu)\n",
    "        expec = get_anomaly_expec(x, t, D, alpha, wires, k, embed_func, transform_func, diag_func, observable,\n",
    "                                   embed_func_params={}, transform_func_params={})\n",
    "        expecs[i] = expec\n",
    "    mean = expecs.mean()\n",
    "    # eta_0 separated from expectation expression since it was an identity matrix prefactor\n",
    "    single_point_cost = (eta_0 - mean)**2/4 # 1/4 factor fom L = 4 in article\n",
    "    return single_point_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\bm{\\mu}$ and $\\bm{\\sigma}$ are the parameters of the normal distribution we sample $\\bm{\\epsilon}$ from: i.e $\\bm{\\epsilon} \\sim \\mathcal{N}(\\bm{\\mu}, \\bm{\\sigma})$. The sampling function is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def sample_M(sigma, mu):\n",
    "    # Sample from a normal distribution\n",
    "    D = torch.normal(mu, sigma.abs())\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the mean over all time-points in the batch, yielding $C_2(\\bm{x_i}, \\bm{\\theta})$ from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_time_series_cost(xt, alpha, eta_0, M_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                         transform_func_params={}, start=0.1, end=2*np.pi):\n",
    "    if t_cycler is None:\n",
    "        t_idxs = np.arange(xt.shape[1])\n",
    "        ts = np.linspace(start, end, xt.shape[1], endpoint=True)\n",
    "    else:\n",
    "        t_idxs = next(t_cycler)\n",
    "        ts = np.linspace(start, end, xt.shape[1], endpoint=True)[t_idxs]\n",
    "    xt_batch = xt[:, t_idxs]\n",
    "    xfunct = zip(xt_batch.T, ts)\n",
    "    a_func_t = \\\n",
    "        [get_single_point_cost(x, t, alpha, eta_0, M_sample_func, sigma, mu, N_E, wires,\n",
    "                               k, embed_func, transform_func, diag_func, observable) for x, t in xfunct]\n",
    "    single_time_series_cost = torch.tensor(a_func_t, requires_grad=True).mean()\n",
    "    return single_time_series_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sum over all time-series in the batch and apply a penalty, yielding the loss function $C(\\bm{\\theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_loss(alpha, eta_0, M_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "            transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau, embed_func_params={},\n",
    "            transform_func_params={}):\n",
    "\n",
    "    X_batch = next(x_cycler)\n",
    "    single_costs = torch.zeros(X_batch.shape[0])\n",
    "    for i, xt in enumerate(X_batch):\n",
    "        single_time_series_cost = \\\n",
    "            get_time_series_cost(xt, alpha, eta_0, M_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                                 transform_func, diag_func, observable, t_cycler, embed_func_params={},\n",
    "                                 transform_func_params={})\n",
    "        single_costs[i] = single_time_series_cost\n",
    "    loss = 0.5*single_costs.mean() + penalty(sigma, tau)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is regularized with a penalty:\n",
    "$$\n",
    "P_{\\bm{\\tau}}(\\bm{\\sigma}) := \\frac{1}{\\pi(Q - 1)} \\sum_{m=1}^{Q - 1} \\arctan(2 \\pi \\tau_m |\\sigma_m|) \n",
    "$$ \n",
    "\n",
    "defined in code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def arctan_penalty(sigma, contraction_hyperparameter):\n",
    "    prefac = 1/(np.pi)\n",
    "    sum_terms = torch.arctan(2*np.pi*contraction_hyperparameter*torch.abs(sigma))\n",
    "    mean = sum_terms.mean()\n",
    "    return prefac*mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find $\\bm{\\theta}^{\\star}$ with classical optimization\n",
    "\n",
    "We now wish to find\n",
    "\n",
    "$$\n",
    "\\bm{\\theta}^{\\star} = \\text{argmin}_{\\bm{\\theta}}[C(\\bm{\\theta})]\n",
    "$$\n",
    "\n",
    "To do so, we first define a helper-class, letting us access optimizers in the `Scipy-optimize` package through the `Pytorch` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        function,\n",
    "        variables,\n",
    "        optimizer_parameters=dict(\n",
    "            method=\"COBYLA\", options={\"disp\": True, \"maxiter\": 10}, jac=False\n",
    "        ),\n",
    "    ):\n",
    "        self.function = function\n",
    "        self.variables = variables\n",
    "        self.optimizer_parameters = optimizer_parameters\n",
    "        self.optimizer = MinimizeWrapper(list(self.variables.values()), self.optimizer_parameters)\n",
    "        self.loss_iterations = []\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "    ):\n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.function(**self.variables)\n",
    "            self.loss_iterations.append(float(loss))\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        self.optimizer.step(closure=closure)\n",
    "        return self.variables\n",
    "\n",
    "    def get_loss_iterations(self):\n",
    "        return self.loss_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the above to optimize `get_loss` within the training `@ct.electron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_model(initial_parameters, M_sample_func, N_E, wires, k, embed_func, n_qubits,\n",
    "                transform_func, diag_func, observable, t_cycler, x_cycler, penalty, tau,\n",
    "                optimizer_params):\n",
    "    alpha = initial_parameters['alpha']\n",
    "    mu = initial_parameters['mu']\n",
    "    sigma = initial_parameters['sigma']\n",
    "    eta_0 = initial_parameters['eta_0']\n",
    "    f = lambda alpha, mu, sigma, eta_0: get_loss(alpha=alpha, mu=mu, sigma=sigma, eta_0=eta_0,\n",
    "                                             M_sample_func=M_sample_func, N_E=N_E, wires=range(n_qubits),\n",
    "                                             k=k, embed_func=embed_func, transform_func=transform_func,\n",
    "                                             diag_func=diag_func, observable=observable,\n",
    "                                             t_cycler=t_cycler, x_cycler=x_cycler,\n",
    "                                             penalty=penalty, tau=tau)\n",
    "    optimizer = Optimizer(f, initial_parameters, optimizer_params)\n",
    "    opt_params = optimizer.optimize()\n",
    "    loss_history = optimizer.get_loss_iterations()\n",
    "    return {\"opt_params\": opt_params, \"loss_history\": loss_history}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last ingredient is defining the initial parameters of the optimization. We do so randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_initial_parameters(transform_func, transform_func_layers, n_qubits, num_distributions):\n",
    "    initial_alpha =\\\n",
    "         torch.tensor(np.random.uniform(0, 2 * np.pi, size=transform_func.shape(transform_func_layers, n_qubits)),\n",
    "         requires_grad=True).type(torch.DoubleTensor)\n",
    "    initial_mu = torch.tensor(\n",
    "                np.random.uniform(0, 2 * np.pi, num_distributions)).type(torch.DoubleTensor)\n",
    "    initial_sigma = torch.tensor(\n",
    "                np.random.uniform(0, 2 * np.pi, num_distributions)).type(torch.DoubleTensor)\n",
    "    initial_eta_0 = torch.tensor(np.random.uniform(-1, 1)).type(torch.DoubleTensor)\n",
    "    init_parameters = {'alpha': initial_alpha, 'mu': initial_mu, 'sigma': initial_sigma, 'eta_0': initial_eta_0}\n",
    "    return init_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We now have all the working parts required to search for $\\bm{\\theta}^{\\star}$. To do so, we stich together the above electrons in a `@ct.lattice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def training_workflow(Xtr_path, n_series_batch, n_t_batch, transform_func, n_qubits,\n",
    "                      transform_func_layers, embed_func, N_E, k, observable, tau, optimizer_params,\n",
    "                      num_distributions):\n",
    "    # load training data\n",
    "    Xtr = load_data_from_pickle(Xtr_path)\n",
    "\n",
    "    # check dimensions of input data are correct\n",
    "    data_santity_check(Xtr)\n",
    "\n",
    "    # Get dataloaders for series and time-point batches\n",
    "    x_cycler = get_series_training_cycler(Xtr, n_series_batch)\n",
    "    t_cycler = get_timepoint_training_cycler(Xtr, n_t_batch)\n",
    "\n",
    "    # get penalty function\n",
    "    penalty = arctan_penalty\n",
    "    \n",
    "    # get sampling function\n",
    "    sampler = sample_M\n",
    "\n",
    "    # get random initial parameters\n",
    "    init_parameters = get_initial_parameters(transform_func, transform_func_layers, n_qubits, num_distributions)\n",
    "\n",
    "    # Run training\n",
    "    opt_results = train_model(init_parameters, sample_M, N_E, range(n_qubits), k, embed_func, n_qubits,\n",
    "                              transform_func, create_diagonal_circuit, observable, t_cycler, x_cycler,\n",
    "                              penalty, tau, optimizer_params)    \n",
    "    return opt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dispatch the `@ct.lattice` to the covalent server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_path = \"\".join([base_path, \"/data/Xtr.pickle\"])\n",
    "dispatch_id = ct.dispatch(training_workflow)(Xtr_path=Xtr_path,\n",
    "                      n_series_batch=10,\n",
    "                      n_t_batch=10,\n",
    "                      num_distributions=3,\n",
    "                      transform_func=qml.templates.StronglyEntanglingLayers,\n",
    "                      n_qubits=2,\n",
    "                      transform_func_layers=3,\n",
    "                      embed_func=qml.templates.AngleEmbedding,\n",
    "                      N_E=10,\n",
    "                      k=2,\n",
    "                      observable=[qml.PauliZ(i) for i in range(n_qubits)],\n",
    "                      tau=5,\n",
    "                      optimizer_params={\"method\": \"Powell\",\n",
    "                                        \"options\": {\"disp\": True, \"maxfev\": 2000,\n",
    "                                                    \"jac\": False, \"maxiter\": 2000}, \"jac\": False})\n",
    "\n",
    "ct_results = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "opt_results = ct_results.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np_norm.minimum.accumulate(opt_results['loss_history']))\n",
    "pickle.dump(opt_results, open('opt_results.pickl', 'wb'))\n",
    "plt.plot(opt_results['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Now with a trained model (and thus $\\bm{\\theta}^{\\star}$), the anomaly score of an unseen time series $\\bm{y}$ can be calculated\n",
    "\n",
    "$$\n",
    "a_X[\\bm{y}] := |C(\\bm{\\theta}^{\\star})  - C_2[\\bm{y}, \\bm{\\theta}^{\\star}]|.\n",
    "$$\n",
    "\n",
    "When $a_X[\\bm{y}] > \\zeta$, $\\bm{y}$ is considered anomalous. Below $\\zeta$, $\\bm{y}$ is considered normal. In validation, using a set of normal instances and a set of labelled anomalous instances, we can tune the threshold $\\zeta$ to maximize some performance metric, which we choose to be the balanced accuracy score.\n",
    "\n",
    "To do any of this, we first need a `@ct.electron` to calculate the anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = pickle.load(open('opt_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_anomaly_scores(Xte, penalty, tau, results_dict, M_sample_func, wires, k, embed_func, N_E,\n",
    "                       transform_func, diag_func, observable):\n",
    "    opt_params = results_dict['opt_params']\n",
    "    alpha = opt_params['alpha']\n",
    "    mu = opt_params['mu']\n",
    "    sigma = opt_params['sigma']\n",
    "    eta_0 = opt_params['eta_0']\n",
    "    normal_cost = np.min(results_dict['loss_history'])\n",
    "    scores = torch.zeros(Xte.shape[0])\n",
    "    for i, yt in enumerate(Xte):\n",
    "        single_time_series_cost =\\\n",
    "            get_time_series_cost(yt, alpha, eta_0, M_sample_func, sigma, mu, N_E, wires, k, embed_func,\n",
    "                         transform_func, diag_func, observable, t_cycler=None)\n",
    "        scores[i] = (normal_cost - single_time_series_cost - penalty(sigma, tau)).abs()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given an arbitary $\\zeta$, we need a function which returns predictions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_label_prediction_pairs(normal_scores, anomalous_scores, threshold):\n",
    "    # -1: normal, 1: anomalous\n",
    "    normal_predictions = [-1 if score.item() < threshold else 1 for score in normal_scores]\n",
    "    normal_labels = [-1 for i in range(len(normal_scores))]\n",
    "\n",
    "    anomalous_predictions = [1 if score.item() >= threshold else -1 for score in anomalous_scores]\n",
    "    anomalous_labels = [1 for i in range(len(anomalous_scores))]\n",
    "\n",
    "    labels = []; labels.extend(normal_labels); labels.extend(anomalous_labels)\n",
    "    predictions = []; predictions.extend(normal_predictions); predictions.extend(anomalous_predictions)\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def balanced_acc_score(truth, prediction):\n",
    "    acc = balanced_accuracy_score(truth, prediction)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and an electron to looop through $\\zeta$ on arbitarily fine grid, and calculate balanced accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def threshold_scan(scores_norm, scores_val, threshold_grid_steps):\n",
    "    zetas = np.linspace(0, 1, threshold_grid_steps)\n",
    "    accs = []\n",
    "    for zeta in zetas:\n",
    "        pred, lab = get_label_prediction_pairs(scores_norm, scores_val, zeta)\n",
    "        acc = balanced_acc_score(lab, pred)\n",
    "        accs.append(acc)\n",
    "    best_idx = np.argmax(accs)\n",
    "    best_zeta = zetas[best_idx]\n",
    "    best_acc = accs[best_idx]\n",
    "    return zetas, accs, best_zeta, best_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a lattice to for the whole validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def validation_workflow(X_norm_path, X_val_path, opt_results, threshold_grid_steps, penalty, tau,\n",
    "                        results_dict, M_sample_func, wires, k, embed_func, N_E, transform_func,\n",
    "                        diag_func, observable):\n",
    "\n",
    "    Xte_norm = load_data_from_pickle(X_norm_path)\n",
    "    Xval = load_data_from_pickle(X_val_path)\n",
    "\n",
    "    # check dimensions of input data are correct\n",
    "    data_santity_check(Xte_norm)\n",
    "    data_santity_check(Xval)\n",
    "    \n",
    "    scores_norm = get_anomaly_scores(Xte_norm, penalty, tau, results_dict,\n",
    "                                 M_sample_func, wires, k, embed_func, N_E,\n",
    "                                 transform_func, diag_func, observable)\n",
    "                                 \n",
    "    scores_val = get_anomaly_scores(Xval, penalty, tau, results_dict,\n",
    "                                    M_sample_func, wires, k, embed_func, N_E,\n",
    "                                    transform_func, diag_func, observable)\n",
    "    \n",
    "    zetas, accs, best_zeta, best_acc = threshold_scan(scores_norm, scores_val, threshold_grid_steps)\n",
    "\n",
    "    return zetas, accs, best_zeta, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = load_data_from_pickle(\"\".join([base_path, \"/data/Xte_norm.pickle\"]))\n",
    "Xte = Xte[:, :, ::4]\n",
    "penalty = arctan_penalty\n",
    "tau = 0.001\n",
    "results_dict = opt_results\n",
    "M_sample_func = sample_M\n",
    "wires = range(n_qubits)\n",
    "k=2\n",
    "embed_func = qml.templates.AngleEmbedding\n",
    "N_E = 5\n",
    "transform_func = qml.templates.StronglyEntanglingLayers\n",
    "diag_func = create_diagonal_circuit\n",
    "observable = [qml.PauliZ(i) for i in range(n_qubits)]\n",
    "scores_norm = get_anomaly_scores(Xte, penalty, tau, results_dict,\n",
    "                                 M_sample_func, wires, k, embed_func, N_E,\n",
    "                                 transform_func, diag_func, observable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = load_data_from_pickle(\"\".join([base_path, \"/data/Xval.pickle\"]))\n",
    "Xte = Xte[:, :, ::4]\n",
    "scores_anom = get_anomaly_scores(Xte, penalty, tau, results_dict,\n",
    "                                 M_sample_func, wires, k, embed_func, N_E,\n",
    "                                 transform_func, diag_func, observable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "zetas = np.linspace(0, 1, 10000)\n",
    "for zeta in zetas:\n",
    "    pred, lab = get_label_prediction_pairs(scores_norm, scores_anom, zeta)\n",
    "    acc = balanced_acc_score(lab, pred)\n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(zetas, accs)\n",
    "plt.xlim(0, 0.1)\n",
    "max_idx = np.argmax(accs)\n",
    "max_acc = accs[max_idx]\n",
    "opt_zeta = zetas[max_idx]\n",
    "print(opt_zeta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now with the optimal $\\zeta$ set from validation, let's see how well we can do with a testing dataset, specifically, $\\tilde{\\mathcal{U}}_+$ from the article.\n",
    "\n",
    "We shall test the balanced accuracy score and the $F_1$ score. This means we need one more electron for the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_F1_score(truth, pred):\n",
    "    score = f1_score(truth, pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need only make a testing workflow, and dispatch it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def testing_workflow(X_norm_path, X_anom_path, best_zeta, opt_results, penalty, tau,\n",
    "                        results_dict, M_sample_func, wires, k, embed_func, N_E, transform_func,\n",
    "                        diag_func, observable):\n",
    "    Xte_norm = load_data_from_pickle(X_norm_path)\n",
    "    Xte_anom = load_data_from_pickle(X_anom_path)\n",
    "\n",
    "    # check dimensions of input data are correct\n",
    "    data_santity_check(Xte_norm)\n",
    "    data_santity_check(Xte_anom)\n",
    "    \n",
    "    scores_norm = get_anomaly_scores(Xte_norm, penalty, tau, results_dict,\n",
    "                                 M_sample_func, wires, k, embed_func, N_E,\n",
    "                                 transform_func, diag_func, observable)\n",
    "                                 \n",
    "    scores_anom = get_anomaly_scores(Xte_anom, penalty, tau, results_dict,\n",
    "                                    M_sample_func, wires, k, embed_func, N_E,\n",
    "                                    transform_func, diag_func, observable)\n",
    "                                    \n",
    "    predictions, labels = get_label_prediction_pairs(scores_norm, scores_anom, best_zeta)\n",
    "\n",
    "    acc_score = balanced_acc_score(labels, predictions)\n",
    "    f1_score = get_F1_score(labels, predictions)\n",
    "\n",
    "    return acc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('QVR')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61006857bd2735ccfd2a6b3291596ddfb06066dbc255c4e4c64dd84bb5990a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
